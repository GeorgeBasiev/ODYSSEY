{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811d71da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "import networkx as nx\n",
    "from rapidfuzz import fuzz\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc36838",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"example_input.json\", \"r\") as f:\n",
    "    example_input = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d829e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_to_df(data_dict):\n",
    "    records = []\n",
    "    for row in data_dict[\"data\"]:\n",
    "        record = {}\n",
    "        for i, (header, _) in enumerate(data_dict[\"header\"]):\n",
    "            text = row[i][0].strip()\n",
    "            link = row[i][1][0] if row[i][1] else None\n",
    "            record[header] = text\n",
    "            record[f\"{header}_link\"] = link\n",
    "        records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "table_df = table_to_df(example_input[\"table\"])\n",
    "table_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66520a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_ent_extr_from_q(question, table_id, table_headers):\n",
    "    return f\"\"\"Agent Introduction: You are an agent who is going to be assisting me in a question answering\n",
    "    task. For this task, I need to first identify the named entities in the question.\n",
    "    Task: Identify the named entities in the provided question. These entities will serve as key elements\n",
    "    for extracting pertinent information from the available sources, which include table name and its\n",
    "    headers.\n",
    "    Output format:\n",
    "    Entities: [‘<entity1>’, ‘<entity2>’, .....]\n",
    "    Use the below example to better understand the task\n",
    "    Input:\n",
    "    Question: What was the nickname of the gold medal winner in the men’s heavyweight Greco-\n",
    "    Roman wrestling event of the 1932 Summer Olympics?\n",
    "    Table Name: Sweden at the 1932 Summer Olympics\n",
    "    Table Headers: [\"Medal\", \"Name\", \"Sport\", \"Event\"]\n",
    "    Output:\n",
    "    Entities: [‘nickname’, ‘medal’, ‘gold’, ‘men’s heavyweight’,\n",
    "    ‘Greco-Roman Wrestling event’, ‘1932 Summer Olympics’]\n",
    "    Input:\n",
    "    Question: {question}\n",
    "    Table Name: {table_id}\n",
    "    Table Headers: {table_headers}\n",
    "    Output:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15499770",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = example_input[\"question\"]\n",
    "table_id = example_input[\"table\"][\"title\"]\n",
    "table_headers = str([i[0] for i in example_input[\"table\"][\"header\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900d46cd",
   "metadata": {},
   "source": [
    "Попробовать запустить без рассуждений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dac9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model=\"qwen3:8b\", \n",
    "    temperature=0,   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7900d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_think_blocks(text):\n",
    "    return re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaf5c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = remove_think_blocks(llm.invoke([HumanMessage(content=prompt_ent_extr_from_q(question, table_id, table_headers))]).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c3b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_relevant_header(question, table_id, table_headers, entities):\n",
    "    return f\"\"\"Agent Introduction: You are an agent who is going to be assisting me in a question answering\n",
    "    task. I have a table as a source of information. I have already extracted the relevant entities from the\n",
    "    question. For this task, I need to first identify the column headers that are relevant in the question.\n",
    "    Task: Identify the relevant column headers from the provided list, based on the extracted entities\n",
    "    from the question. I will also provide the extracted entities from the question and name of the table.\n",
    "    Output format:\n",
    "    Relevant headers: [‘<header-1>’, ‘<header-2>’, ....]\n",
    "    Use the below example to better understand the task\n",
    "    Input:\n",
    "    Question: What was the nickname of the gold medal winner in the men’s heavyweight Greco-\n",
    "    Roman wrestling event of the 1932 Summer Olympics?\n",
    "    Table Name: Sweden at the 1932 Summer Olympics\n",
    "    Table Headers: [\"Medal\", \"Name\", \"Sport\", \"Event\"]\n",
    "    Entities extracted from question: [\"gold medal\", \"men’s heavyweight\", \"Greco-Roman Wrestling\",\n",
    "    \"1932 Summer Olympics\"]\n",
    "    Output:\n",
    "    Relevant headers: [\"Medal\", \"Name\", \"Sport\", \"Event\"]\n",
    "    Input:\n",
    "    Question: {question}\n",
    "    Table Name: {table_id}\n",
    "    Table Headers: {table_headers}\n",
    "    Entities extracted from question: {entities}\n",
    "    Output:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb591943",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_headers = remove_think_blocks(llm.invoke([HumanMessage(content=prompt_relevant_header(question, table_id, table_headers, entities))]).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c703f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_entity_header_mapping(question, table_id, entities, relevant_headers):\n",
    "    return f\"\"\"Agent Introduction: You are an agent who is going to be assisting me in a question answering\n",
    "    task. I have a table as a source of information. I have already extracted relevant entities from the\n",
    "    question and relevant column headers from the table.\n",
    "    Task: Map the entities extracted from the question with the relevant headers and the table name.\n",
    "    Output format:\n",
    "    \"<entity1>\": [\"<mapping1>\", \"<mapping2>\"],\n",
    "    \"<entity2>\": [\"<mapping1>\"]\n",
    "    For each entity extracted from the question, there should be a corresponding <mapping> to an item\n",
    "    in the ‘Relevant headers’ column. If none of the headers match the entity, the mapping should be\n",
    "    labeled as \"Others\".\n",
    "    Use the below example to better understand the task\n",
    "    Input:\n",
    "    Question: What was the nickname of the gold medal winner in the men’s heavyweight Greco-\n",
    "    Roman wrestling event of the 1932 Summer Olympics?\n",
    "    Table Name: Sweden at the 1932 Summer Olympics\n",
    "    Entities extracted from question: [\"gold medal\", \"men’s heavyweight\", \"Greco-Roman Wrestling\",\n",
    "    \"1932 Summer Olympics\"]\n",
    "    Relevant headers: [\"Medal\", \"Name\", \"Sport\", \"Event\"]\n",
    "    Output:\n",
    "    \"gold medal\": [\"Medal\"],\n",
    "    \"men’s heavyweight\": [\"Event\"],\n",
    "    \"Greco-Roman Wrestling\": [\"Sport\"],\n",
    "    \"1932 Summer Olympics\": [\"Others\"]\n",
    "    Input:\n",
    "    Question: {question}\n",
    "    Table Name: {table_id}\n",
    "    Entities extracted from question: {entities}\n",
    "    Relevant Headers: {relevant_headers}\n",
    "    Output:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecba7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_headers_match = remove_think_blocks(llm.invoke([HumanMessage(content=prompt_entity_header_mapping(question, table_id, entities, relevant_headers))]).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3a9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities, relevant_headers, entity_headers_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c937244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_headers = relevant_headers[relevant_headers.index(\"[\")+1:relevant_headers.index(\"]\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129564e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_headers_list = [i.removesuffix(\", \").strip().strip(\"\\\"\") for i in relevant_headers.split(\",\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8100618",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtable_df = table_df[relevant_headers_list + [i + \"_link\" for i in relevant_headers_list]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd98dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_by_doc(\n",
    "    passages,\n",
    "    model=\"en_core_web_trf\",\n",
    "    keep_duplicates=True,\n",
    "):\n",
    "    nlp = spacy.load(model)\n",
    "\n",
    "    out = {}\n",
    "    for doc_id, text in passages.items():\n",
    "        doc = nlp(text or \"\")\n",
    "        ents = []\n",
    "        for e in doc.ents:\n",
    "            ents.append(e.text.strip())\n",
    "        if not keep_duplicates:\n",
    "            seen, uniq = set(), []\n",
    "            for x in ents:\n",
    "                if x not in seen:\n",
    "                    seen.add(x)\n",
    "                    uniq.append(x)\n",
    "            ents = uniq\n",
    "        out[doc_id] = ents\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398df408",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_entities = extract_entities_by_doc(example_input[\"links\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f026ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hybrid_graph(\n",
    "    table_df,\n",
    "    doc_entities,\n",
    "    headers,\n",
    "):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for col in headers:\n",
    "        G.add_node((\"header\", col), kind=\"header\", col=col)\n",
    "\n",
    "    n_rows = len(table_df)\n",
    "    link_suffix = \"_link\"\n",
    "    for r in range(n_rows):\n",
    "        for col in headers:\n",
    "            raw = table_df.loc[r, col] if col in table_df.columns else \"\"\n",
    "            text = \"\" if pd.isna(raw) else str(raw)\n",
    "\n",
    "            link_col = f\"{col}{link_suffix}\"\n",
    "            link_val = table_df.loc[r, link_col] if link_col in table_df.columns else None\n",
    "            link = None if (link_val is None or (isinstance(link_val, float) and pd.isna(link_val)) or str(link_val).strip()==\"\") else str(link_val).strip()\n",
    "\n",
    "            cell = (\"cell\", r, col)\n",
    "            G.add_node(cell, kind=\"cell\", row=r, col=col, text=text, link=link)\n",
    "            G.add_edge(cell, (\"header\", col), kind=\"cell-header\")\n",
    "\n",
    "    for r in range(n_rows):\n",
    "        row_cells = [(\"cell\", r, col) for col in headers]\n",
    "        for u, v in itertools.combinations(row_cells, 2):\n",
    "            if G.has_node(u) and G.has_node(v):\n",
    "                G.add_edge(u, v, kind=\"row\")\n",
    "\n",
    "    def add_ent(ent_text: str):\n",
    "        ent_text = str(ent_text).strip()\n",
    "        if not ent_text:\n",
    "            return None\n",
    "        node = (\"ent\", ent_text)\n",
    "        if not G.has_node(node):\n",
    "            G.add_node(node, kind=\"ent\", text=ent_text)\n",
    "        return node\n",
    "\n",
    "    for node, attr in list(G.nodes(data=True)):\n",
    "        if attr.get(\"kind\") != \"cell\":\n",
    "            continue\n",
    "        link = attr.get(\"link\")\n",
    "        if not link:\n",
    "            continue\n",
    "\n",
    "        ents = doc_entities.get(link, [])\n",
    "        if not ents:\n",
    "            continue\n",
    "\n",
    "        for ent in ents:\n",
    "            ent_node = add_ent(ent)\n",
    "            if ent_node:\n",
    "                G.add_edge(node, ent_node, kind=\"cell-ent\")\n",
    "\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c323e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_graph = build_hybrid_graph(subtable_df, doc_entities, relevant_headers_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9f4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_headers_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d093f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_headers_match_stripped = [i.split(\":\") for i in entity_headers_match.split(\",\\n\")]\n",
    "for i in range(len(entity_headers_match_stripped)):\n",
    "    for j in range(2):\n",
    "        entity_headers_match_stripped[i][j] = entity_headers_match_stripped[i][j].strip(\"\\\"\").strip().strip(\"[\").strip(\"]\").strip(\"\\\"\")\n",
    "\n",
    "entity_headers_dict = {}\n",
    "\n",
    "for i in entity_headers_match_stripped:\n",
    "    entity_headers_dict[i[0]] = i[1]\n",
    "\n",
    "entity_headers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7200c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_instructor_model = None\n",
    "\n",
    "def get_instructor_model():\n",
    "    global _instructor_model\n",
    "    if _instructor_model is None:\n",
    "        _instructor_model = SentenceTransformer('hkunlp/instructor-large')\n",
    "    return _instructor_model\n",
    "\n",
    "def semantic_match(query: str, candidates: list, threshold: float = 0.8):\n",
    "    if not candidates:\n",
    "        return []\n",
    "\n",
    "    model = get_instructor_model()\n",
    "    device = model.device\n",
    "\n",
    "    instruction = \"Represent the question entity for matching with table cells or document entities\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        query_emb = model.encode(\n",
    "            [[instruction, query]],\n",
    "            convert_to_tensor=True,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        candidate_embs = model.encode(\n",
    "            [[instruction, cand] for cand in candidates],\n",
    "            convert_to_tensor=True,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        similarities = util.cos_sim(query_emb, candidate_embs)[0]\n",
    "\n",
    "    matches = []\n",
    "    for i, sim in enumerate(similarities):\n",
    "        if sim >= threshold:\n",
    "            matches.append((candidates[i], float(sim)))\n",
    "    \n",
    "    del query_emb, candidate_embs, similarities\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1562f3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_node_for_output(G, node, headers):\n",
    "    attr = G.nodes[node]\n",
    "    kind = attr.get(\"kind\")\n",
    "    if kind == \"cell\":\n",
    "        text = str(attr.get(\"text\", \"\")).strip()\n",
    "        col = attr.get(\"col\")\n",
    "        if col in headers and text:\n",
    "            return f\"{text}; {col}\"\n",
    "    elif kind == \"ent\":\n",
    "        text = attr.get(\"text\", \"\").strip()\n",
    "        if text:\n",
    "            return text\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5af6076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_and_traverse_hybrid_graph(G, entity_header_mapping, headers, threshold=0.8):\n",
    "    entity_total_list = []\n",
    "    entity_to_node = {}\n",
    "    for node, attr in G.nodes(data=True):\n",
    "        if attr.get(\"kind\") == \"ent\":\n",
    "            text = attr.get(\"text\", \"\").strip()\n",
    "            if text:\n",
    "                entity_total_list.append(text)\n",
    "                entity_to_node[text] = node\n",
    "\n",
    "    def top1_semantic_match(query, candidates, threshold=0.8):\n",
    "        if not candidates:\n",
    "            return None\n",
    "        matches = semantic_match(query, candidates, threshold=threshold)\n",
    "        if not matches:\n",
    "            return None\n",
    "        matches = sorted(matches, key=lambda x: x[1], reverse=True)\n",
    "        cand, score = matches[0]\n",
    "        return (cand, score) if score >= threshold else None\n",
    "\n",
    "    start_nodes = set()\n",
    "\n",
    "    for question_entity, mapped_header in entity_header_mapping.items():\n",
    "        q = str(question_entity).strip()\n",
    "        if not q:\n",
    "            continue\n",
    "\n",
    "        if mapped_header == \"Others\":\n",
    "            tm = top1_semantic_match(q, entity_total_list, threshold)\n",
    "            if tm:\n",
    "                ent_text, _ = tm\n",
    "                node = entity_to_node.get(ent_text)\n",
    "                if node and G.has_node(node):\n",
    "                    start_nodes.add(node)\n",
    "\n",
    "        else:\n",
    "            if mapped_header not in headers:\n",
    "                continue\n",
    "\n",
    "            cell_texts, cell_nodes = [], []\n",
    "            for node, attr in G.nodes(data=True):\n",
    "                if attr.get(\"kind\") == \"cell\" and attr.get(\"col\") == mapped_header:\n",
    "                    text = str(attr.get(\"text\", \"\")).strip()\n",
    "                    if text:\n",
    "                        cell_texts.append(text)\n",
    "                        cell_nodes.append(node)\n",
    "\n",
    "            tm = top1_semantic_match(q, cell_texts, threshold)\n",
    "            if tm:\n",
    "                best_text, _ = tm\n",
    "                try:\n",
    "                    idx = cell_texts.index(best_text)\n",
    "                    start_nodes.add(cell_nodes[idx])\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            else:\n",
    "                all_texts, all_nodes = [], []\n",
    "                for node, attr in G.nodes(data=True):\n",
    "                    if attr.get(\"kind\") == \"cell\":\n",
    "                        t = str(attr.get(\"text\", \"\")).strip()\n",
    "                        if t:\n",
    "                            all_texts.append(t)\n",
    "                            all_nodes.append(node)\n",
    "                tm2 = top1_semantic_match(q, all_texts, threshold)\n",
    "                if tm2:\n",
    "                    best_text, _ = tm2\n",
    "                    try:\n",
    "                        idx = all_texts.index(best_text)\n",
    "                        start_nodes.add(all_nodes[idx])\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "    if not start_nodes:\n",
    "        return None\n",
    "    print(start_nodes)\n",
    "    hop_dict = defaultdict(list)\n",
    "    visited = set(start_nodes)\n",
    "    queue = deque([(node, 0) for node in start_nodes])\n",
    "\n",
    "    while queue:\n",
    "        node, hop = queue.popleft()\n",
    "        if hop >= 3:\n",
    "            continue\n",
    "\n",
    "        next_hop = hop + 1\n",
    "        for neighbor in G.neighbors(node):\n",
    "            if neighbor in visited:\n",
    "                continue\n",
    "\n",
    "            visited.add(neighbor)\n",
    "            queue.append((neighbor, next_hop))\n",
    "\n",
    "            formatted = format_node_for_output(G, neighbor, headers)\n",
    "            if formatted:\n",
    "                hop_dict[f\"{next_hop}-hop\"].append(formatted)\n",
    "\n",
    "    return {\n",
    "        \"1-hop\": hop_dict[\"1-hop\"],\n",
    "        \"2-hop\": hop_dict[\"2-hop\"],\n",
    "        \"3-hop\": hop_dict[\"3-hop\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c83fe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_subheaders = [i for i in list(table_df.columns) if not i.endswith(\"link\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hopwise_context = prune_and_traverse_hybrid_graph(\n",
    "    hybrid_graph,\n",
    "    entity_headers_dict,\n",
    "    headers=original_subheaders\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86836e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hopwise_context[\"3-hop\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dce2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subtable_from_hop(hop_list, table_df):\n",
    "    mentioned_cols = set()\n",
    "    cell_mentions = []\n",
    "\n",
    "    for item in hop_list:\n",
    "        if \"; \" in item:\n",
    "            value_part, col_part = item.rsplit(\"; \", 1)\n",
    "            if col_part in table_df.columns:\n",
    "                mentioned_cols.add(col_part)\n",
    "                cell_mentions.append((value_part.strip(), col_part))\n",
    "\n",
    "    if not mentioned_cols:\n",
    "        return pd.DataFrame(columns=table_df.columns)\n",
    "\n",
    "    cols_to_keep = set(mentioned_cols)\n",
    "    for col in mentioned_cols:\n",
    "        link_col = f\"{col}_link\"\n",
    "        if link_col in table_df.columns:\n",
    "            cols_to_keep.add(link_col)\n",
    "\n",
    "    cols_to_keep = sorted(cols_to_keep, key=lambda x: list(table_df.columns).index(x))  # сохранить порядок\n",
    "\n",
    "    mask = pd.Series([False] * len(table_df), index=table_df.index)\n",
    "    for value, col in cell_mentions:\n",
    "        col_series = table_df[col].astype(str).fillna(\"\")\n",
    "        mask |= (col_series == value)\n",
    "\n",
    "    filtered_df = table_df.loc[mask, cols_to_keep].copy()\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d8dbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_reader_prompt(table_data, passages, question):\n",
    "    return f\"\"\"Agent Introduction: Hello! I’m your Hybrid-QA expert agent, here to assist you in answering\n",
    "    complex questions by leveraging both table data and passage information. Let’s combine these\n",
    "    sources to generate accurate and comprehensive answers!\n",
    "    Task: Your task involves a central question that requires information from both a table and passages.\n",
    "    Here’s the context you’ll need:\n",
    "    Table Data: {table_data}\n",
    "    Passages: {passages}\n",
    "    Question: {question}\n",
    "    Final Answer: Provide the final answer in the format below. If the answer cannot be answered\n",
    "    with the given context, provide None.\n",
    "    Final Answer Format:\n",
    "    Final Answer: <your answer>\n",
    "    If the final answer is \"None\", provide the names of passages that are relevant to the above questions.\n",
    "    If no passages are relevant give ‘[]’ as Relevant Passages.\n",
    "    Relevant Passages Format:\n",
    "    Relevant Passages: [‘<name-of-passage1>’, ‘<name-of-passage2>’, ......]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b025fe48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_links(df):\n",
    "    link_columns = [col for col in df.columns if '_link' in col]\n",
    "\n",
    "    all_links_series = pd.concat([df[col] for col in link_columns], ignore_index=True)\n",
    "    all_links_cleaned = all_links_series.dropna()\n",
    "\n",
    "    return all_links_cleaned.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f35ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop1_table = extract_subtable_from_hop(hopwise_context[\"1-hop\"], subtable_df)\n",
    "hop1_table_md = hop1_table.to_markdown()\n",
    "hop1_links = [example_input[\"links\"][i] for i in collect_links(hop1_table)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0553cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop1_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e06de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_1_invoke = remove_think_blocks(llm.invoke([HumanMessage(content=llm_reader_prompt(hop1_table_md, hop1_links, question))]).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3174e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_1_invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b76181",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 4):\n",
    "    if \"None\" in hop_1_invoke:\n",
    "        relevant_from_hop_1 = hop_1_invoke[hop_1_invoke.index(\"[\")+1:hop_1_invoke.index(\"]\")]\n",
    "        hop2_table = pd.concat([hop1_table, extract_subtable_from_hop(hopwise_context[f\"{i}-hop\"], subtable_df)], ignore_index=True, join=\"inner\").drop_duplicates()\n",
    "\n",
    "        hop2_table_md = hop2_table.to_markdown()\n",
    "        hop2_links = [example_input[\"links\"][i] for i in collect_links(hop2_table)]\n",
    "\n",
    "        if relevant_from_hop_1 != \"\":\n",
    "            relevant_from_hop_1 = relevant_from_hop_1.split(\"\\', \")\n",
    "        hop_2_invoke = remove_think_blocks(llm.invoke([HumanMessage(content=llm_reader_prompt(hop2_table_md, hop2_links, question))]).content)\n",
    "        hop_1_invoke = hop_2_invoke\n",
    "    else:\n",
    "        print(hop_1_invoke)\n",
    "        break\n",
    "\n",
    "\n",
    "if \"None\" in hop_2_invoke:\n",
    "    relevant_from_hop_1 = hop_1_invoke[hop_1_invoke.index(\"[\")+1:hop_1_invoke.index(\"]\")]\n",
    "    hop2_table = pd.concat([hop1_table, extract_subtable_from_hop(hopwise_context[\"2-hop\"], subtable_df)], ignore_index=True)\n",
    "    hop2_table_md = hop2_table.to_markdown()\n",
    "    hop2_links = [example_input[\"links\"][i] for i in collect_links(hop1_table)]\n",
    "\n",
    "    if relevant_from_hop_1 != \"\":\n",
    "        relevant_from_hop_1 = relevant_from_hop_1.split(\"\\', \")\n",
    "        hop2_links = hop2_links + relevant_from_hop_1\n",
    "\n",
    "    hop_2_invoke = remove_think_blocks(llm.invoke([HumanMessage(content=llm_reader_prompt(subtable_df, hop2_links, question))]).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e0bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_1_invoke, hop_2_invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "230d458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f48c894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"preprocessed_for_hybrid_graphs/test_step2_enriched.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94a228c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outputs/test_step2_enriched.json'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"outputs/{os.path.basename(name[:name.index(\".\")])}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de57b3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hybrid_graphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
