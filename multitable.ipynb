{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1b81d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def parse_qa_file(file_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Парсит файл формата Q:/A: в список словарей\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read().strip()\n",
    "    \n",
    "    qa_pairs = []\n",
    "    current_q = None\n",
    "    \n",
    "    for line in content.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        if line.startswith('Q:'):\n",
    "            if current_q is not None:\n",
    "                qa_pairs.append({\"question\": current_q, \"answer\": current_a})\n",
    "            current_q = line[2:].strip()\n",
    "            current_a = \"\"\n",
    "        elif line.startswith('A:'):\n",
    "            current_a = line[2:].strip()\n",
    "    \n",
    "    if current_q is not None:\n",
    "        qa_pairs.append({\"question\": current_q, \"answer\": current_a})\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "def csv_to_table_dict(csv_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Конвертирует CSV файл в формат таблицы для JSON\n",
    "    \"\"\"\n",
    "    # Читаем CSV с сохранением всех типов данных как строк\n",
    "    df = pd.read_csv(csv_path, dtype=str)\n",
    "    \n",
    "    # Название таблицы - имя файла без расширения\n",
    "    table_title = Path(csv_path).stem\n",
    "    \n",
    "    # Форматируем заголовки\n",
    "    headers = [[col, \"\"] for col in df.columns.tolist()]\n",
    "    \n",
    "    # Форматируем данные\n",
    "    data = []\n",
    "    for _, row in df.iterrows():\n",
    "        row_data = []\n",
    "        for col in df.columns:\n",
    "            cell_value = str(row[col]).strip() if pd.notna(row[col]) else \"\"\n",
    "            # Все ячейки без ссылок для pure multitable режима\n",
    "            row_data.append([cell_value, []])\n",
    "        data.append(row_data)\n",
    "    \n",
    "    return {\n",
    "        \"title\": table_title,\n",
    "        \"header\": headers,\n",
    "        \"data\": data\n",
    "    }\n",
    "\n",
    "def create_simple_mapping(qa_pairs: list, csv_files: list) -> list:\n",
    "    \"\"\"\n",
    "    Создает простое сопоставление: один вопрос = одна таблица (по порядку)\n",
    "    \"\"\"\n",
    "    mappings = []\n",
    "    \n",
    "    if len(qa_pairs) != len(csv_files):\n",
    "        print(f\"Внимание: количество вопросов ({len(qa_pairs)}) не совпадает с количеством таблиц ({len(csv_files)})\")\n",
    "        print(\"Будет использовано сопоставление по порядку для первых\", min(len(qa_pairs), len(csv_files)), \"элементов\")\n",
    "    \n",
    "    for i in range(min(len(qa_pairs), len(csv_files))):\n",
    "        mappings.append({\n",
    "            \"question_data\": qa_pairs[i],\n",
    "            \"tables\": [csv_files[i]]\n",
    "        })\n",
    "    \n",
    "    return mappings\n",
    "\n",
    "def create_smart_mapping(qa_pairs: list, csv_files: list, tables_dir: str) -> list:\n",
    "    \"\"\"\n",
    "    Создает \"умное\" сопоставление вопросов и таблиц на основе анализа содержимого\n",
    "    \"\"\"\n",
    "    mappings = []\n",
    "    \n",
    "    # Читаем содержимое всех таблиц для анализа\n",
    "    table_contents = {}\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(tables_dir, csv_file)\n",
    "        df = pd.read_csv(file_path, nrows=5)  # Читаем только первые 5 строк для анализа\n",
    "        table_contents[csv_file] = {\n",
    "            \"columns\": df.columns.tolist(),\n",
    "            \"sample_data\": df.values.flatten().tolist()[:10]  # Первые 10 значений\n",
    "        }\n",
    "    \n",
    "    # Сопоставляем каждый вопрос с наиболее релевантной таблицей\n",
    "    for qa in qa_pairs:\n",
    "        question = qa[\"question\"].lower()\n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for csv_file, content in table_contents.items():\n",
    "            score = 0\n",
    "            \n",
    "            # Анализируем соответствие по названию файла\n",
    "            filename_keywords = re.split(r'[\\s_\\-]+', Path(csv_file).stem.lower())\n",
    "            for keyword in filename_keywords:\n",
    "                if keyword in question:\n",
    "                    score += 2\n",
    "            \n",
    "            # Анализируем соответствие по заголовкам колонок\n",
    "            for col in content[\"columns\"]:\n",
    "                col_clean = re.sub(r'[^\\w\\s]', '', col.lower())\n",
    "                if col_clean in question:\n",
    "                    score += 3\n",
    "            \n",
    "            # Анализируем соответствие по данным таблицы\n",
    "            for value in content[\"sample_data\"]:\n",
    "                if pd.isna(value) or not isinstance(value, str):\n",
    "                    continue\n",
    "                value_clean = re.sub(r'[^\\w\\s]', '', str(value).lower())\n",
    "                if len(value_clean) > 3 and value_clean in question:\n",
    "                    score += 1\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_match = csv_file\n",
    "        \n",
    "        if best_match and best_score > 0:\n",
    "            mappings.append({\n",
    "                \"question_data\": qa,\n",
    "                \"tables\": [best_match]\n",
    "            })\n",
    "        else:\n",
    "            # Если не нашли подходящую таблицу, используем первую доступную\n",
    "            mappings.append({\n",
    "                \"question_data\": qa,\n",
    "                \"tables\": [csv_files[0]] if csv_files else []\n",
    "            })\n",
    "    \n",
    "    return mappings\n",
    "\n",
    "def create_multitable_mapping(qa_pairs: list, csv_files: list) -> list:\n",
    "    \"\"\"\n",
    "    Создает сопоставление для multitable-сценария: один вопрос = все таблицы\n",
    "    \"\"\"\n",
    "    return [{\n",
    "        \"question_data\": qa,\n",
    "        \"tables\": csv_files.copy()\n",
    "    } for qa in qa_pairs]\n",
    "\n",
    "def generate_json_files(\n",
    "    qa_file: str, \n",
    "    tables_dir: str, \n",
    "    output_dir: str,\n",
    "    mode: str = \"smart\"  # simple, smart, multitable\n",
    "):\n",
    "    \"\"\"\n",
    "    Основная функция генерации JSON файлов\n",
    "    \"\"\"\n",
    "    # Создаем выходную директорию\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Парсим вопросы и ответы\n",
    "    qa_pairs = parse_qa_file(qa_file)\n",
    "    if not qa_pairs:\n",
    "        print(\"Не найдено вопросов в файле!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Найдено вопросов: {len(qa_pairs)}\")\n",
    "    \n",
    "    # Получаем список CSV файлов\n",
    "    csv_files = [f for f in os.listdir(tables_dir) if f.endswith('.csv')]\n",
    "    if not csv_files:\n",
    "        print(\"Не найдено CSV файлов в директории!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Найдено таблиц: {len(csv_files)}\")\n",
    "    \n",
    "    # Выбираем режим сопоставления\n",
    "    if mode == \"simple\":\n",
    "        mappings = create_simple_mapping(qa_pairs, csv_files)\n",
    "    elif mode == \"multitable\":\n",
    "        mappings = create_multitable_mapping(qa_pairs, csv_files)\n",
    "    else:  # smart mode (по умолчанию)\n",
    "        mappings = create_smart_mapping(qa_pairs, csv_files, tables_dir)\n",
    "    \n",
    "    # Генерируем JSON файлы\n",
    "    for i, mapping in enumerate(mappings):\n",
    "        question = mapping[\"question_data\"][\"question\"]\n",
    "        tables = mapping[\"tables\"]\n",
    "        \n",
    "        if not tables:\n",
    "            print(f\"Пропущен вопрос '{question}': нет подходящих таблиц\")\n",
    "            continue\n",
    "        \n",
    "        # Обрабатываем одну или несколько таблиц\n",
    "        table_data = []\n",
    "        for table_file in tables:\n",
    "            csv_path = os.path.join(tables_dir, table_file)\n",
    "            table_dict = csv_to_table_dict(csv_path)\n",
    "            table_data.append(table_dict)\n",
    "        \n",
    "        # Формируем итоговый JSON\n",
    "        if len(tables) == 1:\n",
    "            # Single-table режим\n",
    "            output_json = {\n",
    "                \"question\": question,\n",
    "                \"table\": table_data[0],\n",
    "                # Для pure multitable режима поле \"links\" отсутствует\n",
    "            }\n",
    "        else:\n",
    "            # Multitable режим\n",
    "            output_json = {\n",
    "                \"question\": question,\n",
    "                \"tables\": table_data,  # Массив таблиц\n",
    "                # Для pure multitable режима поле \"links\" отсутствует\n",
    "            }\n",
    "        \n",
    "        # Сохраняем в файл\n",
    "        safe_question = re.sub(r'[^\\w\\s\\-]', '', question)[:50].strip().replace(' ', '_')\n",
    "        output_path = os.path.join(output_dir, f\"q{i+1}_{safe_question}.json\")\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output_json, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"Создан файл: {os.path.basename(output_path)}\")\n",
    "        print(f\"  Вопрос: {question}\")\n",
    "        print(f\"  Таблицы: {', '.join(tables)}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"Всего создано JSON файлов: {len(mappings)}\")\n",
    "    print(f\"Файлы сохранены в: {os.path.abspath(output_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2691db0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "db_path = \"/home/george/Documents/code/vkr/rustbench/tables-qa/set/northwind/3/local_snapshot.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Получаем список всех таблиц\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "\n",
    "for table_name in tables:\n",
    "    table_name = table_name[0]\n",
    "    table = pd.read_sql_query(f'SELECT * FROM \"{table_name}\"', conn)\n",
    "    table.to_csv(f\"/home/george/Documents/code/vkr/HybridGraphs/json_inputs/northwind3-sample/tables/{table_name}.csv\", index=False)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e60d236",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_dir = \"json_inputs/northwind3-mini/tables/\"\n",
    "\n",
    "for name in os.listdir(tables_dir):\n",
    "    pd.read_csv(tables_dir + name).iloc[:2].to_csv(tables_dir + name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e770353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено вопросов: 1\n",
      "Найдено таблиц: 30\n",
      "Создан файл: q1_How_many_unique_orders_were_placed_by_US_customers.json\n",
      "  Вопрос: How many unique orders were placed by US customers that were processed by Seattle-based employees?\n",
      "  Таблицы: order subtotals.csv, sqlite_sequence.csv, category sales for 1997.csv, summary of sales by year.csv, products above average price.csv, orders.csv, employees.csv, employeeterritories.csv, shippers.csv, alphabetical list of products.csv, order details.csv, products.csv, sales totals by amount.csv, orders qry.csv, order details extended.csv, current product list.csv, product sales for 1997.csv, sales by category.csv, customercustomerdemo.csv, products by category.csv, invoices.csv, categories.csv, suppliers.csv, customer and suppliers by city.csv, quarterly orders.csv, customerdemographics.csv, territories.csv, summary of sales by quarter.csv, customers.csv, region.csv\n",
      "\n",
      "Всего создано JSON файлов: 1\n",
      "Файлы сохранены в: /home/george/Documents/code/vkr/HybridGraphs/json_inputs\n"
     ]
    }
   ],
   "source": [
    "QA_FILE = \"json_inputs/northwind3-sample/QA.txt\"          # Путь к файлу с вопросами\n",
    "TABLES_DIR = \"json_inputs/northwind3-sample/tables\"              # Папка с CSV файлами\n",
    "OUTPUT_DIR = \"json_inputs\"         # Папка для выходных JSON файлов\n",
    "MODE = \"multitable\"                     # Режим сопоставления: simple, smart, multitable\n",
    "\n",
    "generate_json_files(\n",
    "    qa_file=QA_FILE,\n",
    "    tables_dir=TABLES_DIR,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    mode=MODE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fad862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hybrid_graphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
